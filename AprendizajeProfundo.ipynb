{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6421c87",
   "metadata": {},
   "source": [
    "**Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones**\n",
    "\n",
    "**Edición 2022**\n",
    "\n",
    "---\n",
    "## **Aprendizaje Automático Profundo - Deep Learning**\n",
    "---\n",
    "\n",
    "## **Trabajo Práctico Entregable 1 - Red Neuronal Simple**\n",
    "\n",
    "**Integrantes:** Gastón Briozzo, Tomás Niño Kehoe\n",
    "\n",
    "**Profesores:** Johanna Frau,  Mauricio Mazuecos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c3f55b",
   "metadata": {},
   "source": [
    "# Importamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c36c6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn import metrics\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import functools\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tempfile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, balanced_accuracy_score\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing import preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "#from torchmetrics import AveragePrecision\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc3b9dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1+cu111'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d42351f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿CUDA disponible? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"¿CUDA disponible? {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acece3d9",
   "metadata": {},
   "source": [
    "## Exploración de datos\n",
    "Cargamos los datos de entrenamiento para ver que forma tienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a2f272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_json('./data/meli-challenge-2019/spanish.validation.jsonl.gz',lines=True)\n",
    "#No usar para visualizar, muy pesado. Usar test o val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4c19e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "      <th>n_labels</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Metal Biela Dw10 Hdi 2.0</td>\n",
       "      <td>ENGINE_BEARINGS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[metal, biela, hdi]</td>\n",
       "      <td>[457, 1480, 3450]</td>\n",
       "      <td>88</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Repuestos Martillo Rotoprcutor Bosch Gshsce Po...</td>\n",
       "      <td>ELECTRIC_DEMOLITION_HAMMERS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[repuestos, martillo, rotoprcutor, bosch, gshs...</td>\n",
       "      <td>[3119, 892, 1, 767, 1, 9337]</td>\n",
       "      <td>174</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Pesca Caña Pejerrey Colony Brava 3m Fibra De V...</td>\n",
       "      <td>FISHING_RODS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[pesca, caña, pejerrey, colony, brava, fibra, ...</td>\n",
       "      <td>[700, 990, 2057, 3990, 3670, 1737, 1153, 6568]</td>\n",
       "      <td>313</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Porcelanato Abitare Be 20x120 Cm. Ceramica Por...</td>\n",
       "      <td>PORCELAIN_TILES</td>\n",
       "      <td>validation</td>\n",
       "      <td>[porcelanato, abitare, ceramica, portinari]</td>\n",
       "      <td>[2722, 4404, 1406, 4405]</td>\n",
       "      <td>427</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Reconstruction Semi Di Lino Alfaparf Shampoo 1...</td>\n",
       "      <td>HAIR_SHAMPOOS_AND_CONDITIONERS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[reconstruction, semi, lino, alfaparf, shampoo]</td>\n",
       "      <td>[1, 3365, 7502, 10919, 849]</td>\n",
       "      <td>194</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Mascara Fotosensible Lüsqtoff, Oferta Y En Cuo...</td>\n",
       "      <td>WELDING_MASKS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[mascara, fotosensible, lüsqtoff, oferta, cuotas]</td>\n",
       "      <td>[846, 2521, 38384, 446, 769]</td>\n",
       "      <td>407</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Bermuda John Cena 14/16 Original</td>\n",
       "      <td>SHORTS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[bermuda, john, cena, original]</td>\n",
       "      <td>[998, 2168, 5925, 188]</td>\n",
       "      <td>198</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>20x Rueda Neumático Tuerca Set De 20 Lr068126 ...</td>\n",
       "      <td>MOTORCYCLE_WHEEL_AXLES</td>\n",
       "      <td>validation</td>\n",
       "      <td>[rueda, neumático, tuerca, set, oem, tierr]</td>\n",
       "      <td>[458, 1317, 11808, 489, 859, 1]</td>\n",
       "      <td>578</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Pelota De Basquet Spalding Tf-elite Nº 6</td>\n",
       "      <td>BASKET_BALLS</td>\n",
       "      <td>validation</td>\n",
       "      <td>[pelota, basquet, spalding, elite]</td>\n",
       "      <td>[1211, 953, 6649, 3377]</td>\n",
       "      <td>187</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Placard De Algarrobo Original 3 Puertas</td>\n",
       "      <td>WARDROBES</td>\n",
       "      <td>validation</td>\n",
       "      <td>[placard, algarrobo, original, puertas]</td>\n",
       "      <td>[1537, 3898, 188, 3616]</td>\n",
       "      <td>292</td>\n",
       "      <td>632</td>\n",
       "      <td>1223820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language label_quality                                              title  \\\n",
       "0  spanish    unreliable                           Metal Biela Dw10 Hdi 2.0   \n",
       "1  spanish    unreliable  Repuestos Martillo Rotoprcutor Bosch Gshsce Po...   \n",
       "2  spanish    unreliable  Pesca Caña Pejerrey Colony Brava 3m Fibra De V...   \n",
       "3  spanish    unreliable  Porcelanato Abitare Be 20x120 Cm. Ceramica Por...   \n",
       "4  spanish    unreliable  Reconstruction Semi Di Lino Alfaparf Shampoo 1...   \n",
       "5  spanish    unreliable  Mascara Fotosensible Lüsqtoff, Oferta Y En Cuo...   \n",
       "6  spanish    unreliable                   Bermuda John Cena 14/16 Original   \n",
       "7  spanish    unreliable  20x Rueda Neumático Tuerca Set De 20 Lr068126 ...   \n",
       "8  spanish    unreliable           Pelota De Basquet Spalding Tf-elite Nº 6   \n",
       "9  spanish    unreliable           Placard De Algarrobo Original 3 Puertas    \n",
       "\n",
       "                         category       split  \\\n",
       "0                 ENGINE_BEARINGS  validation   \n",
       "1     ELECTRIC_DEMOLITION_HAMMERS  validation   \n",
       "2                    FISHING_RODS  validation   \n",
       "3                 PORCELAIN_TILES  validation   \n",
       "4  HAIR_SHAMPOOS_AND_CONDITIONERS  validation   \n",
       "5                   WELDING_MASKS  validation   \n",
       "6                          SHORTS  validation   \n",
       "7          MOTORCYCLE_WHEEL_AXLES  validation   \n",
       "8                    BASKET_BALLS  validation   \n",
       "9                       WARDROBES  validation   \n",
       "\n",
       "                                     tokenized_title  \\\n",
       "0                                [metal, biela, hdi]   \n",
       "1  [repuestos, martillo, rotoprcutor, bosch, gshs...   \n",
       "2  [pesca, caña, pejerrey, colony, brava, fibra, ...   \n",
       "3        [porcelanato, abitare, ceramica, portinari]   \n",
       "4    [reconstruction, semi, lino, alfaparf, shampoo]   \n",
       "5  [mascara, fotosensible, lüsqtoff, oferta, cuotas]   \n",
       "6                    [bermuda, john, cena, original]   \n",
       "7        [rueda, neumático, tuerca, set, oem, tierr]   \n",
       "8                 [pelota, basquet, spalding, elite]   \n",
       "9            [placard, algarrobo, original, puertas]   \n",
       "\n",
       "                                             data  target  n_labels     size  \n",
       "0                               [457, 1480, 3450]      88       632  1223820  \n",
       "1                    [3119, 892, 1, 767, 1, 9337]     174       632  1223820  \n",
       "2  [700, 990, 2057, 3990, 3670, 1737, 1153, 6568]     313       632  1223820  \n",
       "3                        [2722, 4404, 1406, 4405]     427       632  1223820  \n",
       "4                     [1, 3365, 7502, 10919, 849]     194       632  1223820  \n",
       "5                    [846, 2521, 38384, 446, 769]     407       632  1223820  \n",
       "6                          [998, 2168, 5925, 188]     198       632  1223820  \n",
       "7                 [458, 1317, 11808, 489, 859, 1]     578       632  1223820  \n",
       "8                         [1211, 953, 6649, 3377]     187       632  1223820  \n",
       "9                         [1537, 3898, 188, 3616]     292       632  1223820  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3106307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c98af1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.label_quality.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc03df",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Tomado de https://github.com/DiploDatos/AprendizajeProfundo/blob/master/3_datasets.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce1231",
   "metadata": {},
   "source": [
    "### Esta clase codifica el Target. \n",
    "Éste pasa de ser una variable unidimensional discretizada con valores de 1 a 632 \n",
    "a una variable con 632 dimensiónes, cada una con valor 0 o 1.\n",
    "\n",
    "Esta transformación es necesaria para la posterior implementación de la red neuronal,\n",
    "dado que si el target se presenta como un valor ordinal, la red interpretará que este\n",
    "orden tiene relevancia. \n",
    "Para evitar esto, cada categoría en el target debe tener su propia columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "428e51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiChallengeDataset(IterableDataset):\n",
    "    def __init__(self, path, transform=None, key = 'data'):\n",
    "        \"\"\"\n",
    "        path: Ubicación a los datos (comprimidos con gzip)\n",
    "        key: Columna que vamos a usar para entrenar\n",
    "        \"\"\"\n",
    "        self.dataset_path = path\n",
    "        self.transform = transform\n",
    "        self.key = key\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Habilita un iterador sobre los datos\n",
    "        \"\"\"\n",
    "        with gzip.open(self.dataset_path, \"rt\") as fh:\n",
    "            for l in fh:\n",
    "                data = json.loads(l)    \n",
    "                \n",
    "                # one-hot encoding like.\n",
    "                index = int(data['target'])\n",
    "                encoded_target = np.zeros(632)\n",
    "                encoded_target[index] = 1\n",
    "\n",
    "                item = {\n",
    "                    \"data\": data[self.key],\n",
    "#                     \"target\": data['target']\n",
    "                    \"target\": encoded_target #data['target']\n",
    "                }\n",
    "                \n",
    "                if self.transform:\n",
    "                    yield self.transform(item)\n",
    "                else:\n",
    "                    yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0461df0",
   "metadata": {},
   "source": [
    "### Preprocesamiento  (NO NECESARIO SI USAMOS DATA)\n",
    "Este se encargará de preprocesar y normalizar el texto. Las palabras se transformarán  en índices de un diccionario, se creará un vector con la secuencia de palabras y se lo pasará a la matriz de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fb85c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class RawDataProcessor:\n",
    "#    def __init__(self, \n",
    "#                 dataset, \n",
    "#                 ignore_header=True, \n",
    "#                 filters=None, \n",
    "#                 vocab_size=50001):\n",
    "#        if filters:\n",
    "#            self.filters = filters\n",
    "#        else:\n",
    "#            self.filters = [\n",
    "#                lambda s: s.lower(),\n",
    "#                preprocessing.strip_tags,\n",
    "#                preprocessing.strip_punctuation,\n",
    "#                preprocessing.strip_multiple_whitespaces,\n",
    "#                preprocessing.strip_numeric,\n",
    "#                preprocessing.remove_stopwords,\n",
    "#                preprocessing.strip_short,\n",
    "#            ]\n",
    "#        \n",
    "#        # Create dictionary based on all the reviews (with corresponding preprocessing)\n",
    "#        # https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "#        self.dictionary = corpora.Dictionary(\n",
    "#            dataset[\"tokenized_title\"].map(self._preprocess_string).tolist()\n",
    "#        )\n",
    "#        # Filter the dictionary with extremos words\n",
    "#        # https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.filter_extremes.html?highlight=filter_extrem\n",
    "#        self.dictionary.filter_extremes(no_below=2, no_above=1, keep_n=vocab_size)\n",
    "#        \n",
    "#        # Make the indices continuous after some words have been removed\n",
    "#        # https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.compactify.html\n",
    "#        self.dictionary.compactify()\n",
    "#        \n",
    "#        # Add a couple of special tokens\n",
    "#        self.dictionary.patch_with_special_tokens({\n",
    "#            \"[PAD]\": 0,\n",
    "#            \"[UNK]\": 1\n",
    "#        })\n",
    "#        self.idx_to_target = sorted(dataset[\"target\"].unique())\n",
    "#        self.target_to_idx = {t: i for i, t in enumerate(self.idx_to_target)}\n",
    "#\n",
    "#\n",
    "#    def _preprocess_string(self, string):\n",
    "#        # https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.preprocess_string:~:text=gensim.parsing.preprocessing.preprocess_string\n",
    "#        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "#\n",
    "#    def _sentence_to_indices(self, sentence):\n",
    "#      # https://radimrehurek.com/gensim/corpora/dictionary.html#:~:text=doc2idx(document,via%20unknown_word_index.\n",
    "#        return self.dictionary.doc2idx(sentence, unknown_word_index=1)\n",
    "#    \n",
    "#    def encode_data(self, data):\n",
    "#        return self._sentence_to_indices(self._preprocess_string(data))\n",
    "#    \n",
    "#    def encode_target(self, target):\n",
    "#        return self.target_to_idx[target]\n",
    "#    \n",
    "#    def __call__(self, item):\n",
    "#        if isinstance(item[\"data\"], str):\n",
    "#            data = self.encode_data(item[\"data\"])\n",
    "#        else:\n",
    "#            data = [self.encode_data(d) for d in item[\"data\"]]\n",
    "#        \n",
    "#        if isinstance(item[\"target\"], str):\n",
    "#            target = self.encode_target(item[\"target\"])\n",
    "#        else:\n",
    "#            target = [self.encode_target(t) for t in item[\"target\"]]\n",
    "#        \n",
    "#        return {\n",
    "#            \"data\": data,\n",
    "#            \"target\": target\n",
    "#        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e5699",
   "metadata": {},
   "source": [
    "### Lectura de datos (NO EJECUTAR SI USAMOS DATA)\n",
    "Este se encargará de preprocesar y normalizar el texto. Las palabras se transformarán  en índices de un diccionario, se creará un vector con la secuencia de palabras y se lo pasará a la matriz de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "837dce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.read_json('./data/meli-challenge-2019/spanish.train.jsonl.gz',lines=True)\n",
    "\n",
    "#preprocess = RawDataProcessor(dataset)\n",
    "\n",
    "#train_indices, test_indices = train_test_split(dataset.index, test_size=0.2, random_state=42)\n",
    "\n",
    "#train_dataset = IMDBReviewsDataset(dataset.loc[train_indices].reset_index(drop=True), transform=preprocess)\n",
    "\n",
    "#test_dataset = IMDBReviewsDataset(dataset.loc[test_indices].reset_index(drop=True), transform=preprocess)\n",
    "\n",
    "#print(f\"Datasets loaded with {len(train_dataset)} training elements and {len(test_dataset)} test elements\")\n",
    "#print(f\"Sample train element:\\n{train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c1ff4",
   "metadata": {},
   "source": [
    "### Esta clase estandariza las entradas. \n",
    "Las entradas son vectores cuya dimensión se corresponde con la cantidad de palabras empleadas para describir el artículo. Los valores en cada dimensión vienen dados por la palabra correspondiente.\n",
    "Esta clase añade dimensiones al vector de entrada de modo que todos tengan la misma dimensionalidad (20).\n",
    "\n",
    "Esta transformación es necesaria para la posterior implementación de la red neuronal,\n",
    "dado que estas requieren que las entradas que alimentarán el modelo tengan la misma dimensionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bd1d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "        \n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "            \n",
    "        return {\n",
    "           \"data\": torch.LongTensor(data),\n",
    "#             \"data\": torch.FloatTensor(data), \n",
    "            \"target\": torch.FloatTensor(target)\n",
    "            \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2ce9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una collate_fn que nos retorne todos los arreglos de la misma longitud\n",
    "# data_len = train_dataset.data.apply(lambda v: len(v))\n",
    "# max_len = data_len.max()\n",
    "\n",
    "\n",
    "pad_to_len = PadSequences(max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e15a3",
   "metadata": {},
   "source": [
    "### Juntando todo\n",
    "Tenemos\n",
    "* Una clase con la responsabilidad de entregar datos, potencialmente preprocesandolos si hace falta\n",
    "* Una función que transforma los datos leidos para que todos los valores tengan la misma longitud\n",
    "\n",
    "A partir de esto, creamos dos instancia del `DataLoader`: Uno para cargar los datos de entrenamiento y otro para cargar los datos de _test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16108441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MeLiChallengeDataset('./data/meli-challenge-2019/spanish.train.jsonl.gz', key = 'data')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False, num_workers=0, collate_fn = pad_to_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52d91f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MeLiChallengeDataset('./data/meli-challenge-2019/spanish.validation.jsonl.gz', key = 'data') \n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0, collate_fn = pad_to_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc4460",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "Tomamos el MLP de las clases y agregamos los parametros que nos interesan\n",
    "\n",
    "* Capa de entrada: Tantas neuronas como tokens\n",
    "* Capa de salida: Tantas neuronas como categorías (632)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc7cfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'casita' in dictionary:\n",
    "#     print('bien')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adcd747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0140e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden, \n",
    "                 pretrained_embeddings_path = \"./data/SBW-vectors-300-min5.txt.bz2\", \n",
    "                 dictionary_path            = \"./data/meli-challenge-2019/spanish_token_to_index.json.gz\",#preprocess.dictionary,\n",
    "                 freeze_embedings           = True):\n",
    "        \"\"\"\n",
    "        input_size: Número de neuronas de entrada\n",
    "        output_size: Número de neuronas de salida\n",
    "        hidden: Lista con los numeros de capas ocultas\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(hidden) > 0\n",
    "        \n",
    "        self._name = str(input_size) +'_'+ \"_\".join(map(lambda i: str(i), hidden)) + '_' + str(output_size)\n",
    "        \n",
    "        f = gzip.open(dictionary_path, \"rt\")\n",
    "        raw_json = f.read()\n",
    "        dictionary = json.loads(raw_json)\n",
    "        dictionary\n",
    "        \n",
    "        embeddings_matrix = torch.randn(len(dictionary), input_size)\n",
    "        embeddings_matrix[0] = torch.zeros(input_size)\n",
    "        \n",
    "        with bz2.open(pretrained_embeddings_path, \"rt\") as fh:\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                \n",
    "                if word in dictionary:\n",
    "                    embeddings_matrix[dictionary[word]] =\\\n",
    "                        torch.FloatTensor([float(n) for n in vector.split()])\n",
    "                    \n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "                                                       freeze=freeze_embedings,\n",
    "                                                       padding_idx=0)\n",
    "        \n",
    "        ## Set up layers\n",
    "        neurons = [input_size]  + hidden \n",
    "        parts = []\n",
    "        for idx, each in enumerate(neurons[:-1]):\n",
    "            parts.append(nn.Linear(each, neurons[idx + 1]))\n",
    "            parts.append(nn.ReLU())\n",
    "            \n",
    "        parts = parts + [nn.Linear(neurons[-1], output_size), nn.Sigmoid()]\n",
    "        \n",
    "        self.model = nn.Sequential(*parts)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MLP_\" + self._name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8817ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ESTA CELDA NO VA; EXTRAER EMBEDDING DE AQUI PARA CELDA ANTERIOR\n",
    "#class IMDBReviewsClassifier(nn.Module):\n",
    "#    def __init__(self, \n",
    "#                 pretrained_embeddings_path, \n",
    "#                 dictionary,\n",
    "#                 vector_size,\n",
    "#                 freeze_embedings):\n",
    "#        super().__init__()\n",
    "#        embeddings_matrix = torch.randn(len(dictionary), vector_size)\n",
    "#        embeddings_matrix[0] = torch.zeros(vector_size)\n",
    "#        with gzip.open(pretrained_embeddings_path, \"rt\") as fh:\n",
    "#            for line in fh:\n",
    "#                word, vector = line.strip().split(None, 1)\n",
    "#                if word in dictionary.token2id:\n",
    "#                    embeddings_matrix[dictionary.token2id[word]] =\\\n",
    "#                        torch.FloatTensor([float(n) for n in vector.split()])\n",
    "#        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "#                                                       freeze=freeze_embedings,\n",
    "#                                                       padding_idx=0)\n",
    "#        self.hidden1 = nn.Linear(vector_size, 128)\n",
    "#        self.hidden2 = nn.Linear(128, 128)\n",
    "#        self.output = nn.Linear(128, 1)\n",
    "#        self.vector_size = vector_size\n",
    "#    \n",
    "#    def forward(self, x):\n",
    "#        x = self.embeddings(x)\n",
    "#        x = torch.mean(x, dim=1)\n",
    "#        x = F.relu(self.hidden1(x))\n",
    "#        x = F.relu(self.hidden2(x))\n",
    "#        x = torch.sigmoid(self.output(x))\n",
    "#        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a03b08c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2416780/3450571657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_embeddings_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/SBW-vectors-300-min5.txt.bz2\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2416780/3764895675.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, output_size, hidden, pretrained_embeddings_path, dictionary_path, freeze_embedings)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_embeddings_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.9/bz2.py\u001b[0m in \u001b[0;36mread1\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.9/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.9/_compression.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0mrawblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MLP(300, 2 , hidden = [16], pretrained_embeddings_path = \"./data/SBW-vectors-300-min5.txt.bz2\" ).name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60ce89f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (embeddings): Embedding(50002, 300, padding_idx=0)\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=632, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP(300,632,[256])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c773c0",
   "metadata": {},
   "source": [
    "### Armado paso a paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe899a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias para obtener los datos en batch\n",
    "get_train_dataloader = lambda bs: DataLoader(train_dataset, batch_size=bs, shuffle=False, num_workers=0, collate_fn = pad_to_len)\n",
    "get_test_dataloader = lambda bs: DataLoader(test_dataset, batch_size=bs, shuffle=False, num_workers=0, collate_fn = pad_to_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3687c",
   "metadata": {},
   "source": [
    "#### Verificamos la forma de la entrada, salida y la etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dff8f378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50001,     2, 50000,  ...,     0,     0,     0],\n",
       "         [    6,     4,     5,  ...,     0,     0,     0],\n",
       "         [    9,     7,    10,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  532,   398,   530,  ...,     0,     0,     0],\n",
       "         [  534,   536,     1,  ...,     0,     0,     0],\n",
       "         [  370,    76,   539,  ...,     0,     0,     0]]),\n",
       " tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(train_dataloader)\n",
    "e = it.next()\n",
    "data, target = e['data'], e['target']\n",
    "data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b296f522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 20]), torch.Size([128, 632]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size(), target.size() # batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "131862fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50001,     2, 50000,  ...,     0,     0,     0],\n",
       "        [    6,     4,     5,  ...,     0,     0,     0],\n",
       "        [    9,     7,    10,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  532,   398,   530,  ...,     0,     0,     0],\n",
       "        [  534,   536,     1,  ...,     0,     0,     0],\n",
       "        [  370,    76,   539,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c2fe574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5081, 0.5064, 0.5086,  ..., 0.4846, 0.5042, 0.5128],\n",
       "         [0.5085, 0.5062, 0.5091,  ..., 0.4853, 0.5048, 0.5119],\n",
       "         [0.5097, 0.5060, 0.5085,  ..., 0.4854, 0.5053, 0.5119],\n",
       "         ...,\n",
       "         [0.5087, 0.5063, 0.5086,  ..., 0.4855, 0.5046, 0.5121],\n",
       "         [0.5098, 0.5063, 0.5105,  ..., 0.4869, 0.5051, 0.5121],\n",
       "         [0.5095, 0.5062, 0.5084,  ..., 0.4861, 0.5051, 0.5121]],\n",
       "        grad_fn=<SigmoidBackward0>),\n",
       " tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = MLP(300, 632,[256])\n",
    "output = m(data)\n",
    "output, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "797bc1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertimos a la forma esperada\n",
    "#cmp = lambda t: torch.Tensor([[x] for x in t])\n",
    "cmp = lambda t: t #ID\n",
    "actual = cmp(target)\n",
    "actual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ee7cf",
   "metadata": {},
   "source": [
    "#### Verificamos que se pueda calcular la función de pérdida y el _score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e8629c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6923, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.BCELoss()\n",
    "loss_value = loss(output, actual)\n",
    "loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "06c8cca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  21,\n",
       "        38,  39,  40,  41,  42,  43,  29,  44,  45,  46,  47,  48,  49,\n",
       "        50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,\n",
       "        45,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  71,\n",
       "        74,  75,  76,  77,  78,  76,  79,  80,  81,  27,  82,  83,  39,\n",
       "        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,\n",
       "        79,  97,  98,  34,  99,  90,  91, 100, 101, 102, 103, 104, 105,\n",
       "       106, 107,  86,  78,  77, 108, 109, 110,  85,  26, 111])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_from_tensor = lambda t: torch.max(t.data, 1)[1]\n",
    "y_true = categories_from_tensor(target)\n",
    "y_true.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e0cafd09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([369, 369, 369, 531,  37,  48, 369,  48, 531, 369, 156, 496, 369,\n",
       "        369, 531, 531, 531,  48, 531, 531, 520, 531, 531,  48, 531,  17,\n",
       "        369, 604, 189, 369, 531, 531, 369, 369, 369, 531, 531, 604, 369,\n",
       "        531, 531,  57, 369, 531, 496, 531, 369,  48, 531,  48, 520, 369,\n",
       "        531, 189,  57, 369, 369, 491, 531, 520, 531, 369, 369, 461, 531,\n",
       "        534, 531, 520, 531, 369, 531, 531, 369, 381, 604, 369, 369, 369,\n",
       "        369, 369, 369, 369, 369, 105, 531, 531, 369, 531, 604, 369, 531,\n",
       "         57,  48, 604,  48, 369, 369, 369, 369, 531, 369, 369,  48, 369,\n",
       "         48, 531, 531,  72, 381, 369, 520, 531, 531, 369, 531, 369, 491,\n",
       "        531, 520, 520, 369, 369, 369,  57, 461, 369, 531, 531]),\n",
       " array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = categories_from_tensor(output)\n",
    "predicted.numpy(), target.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d8a341c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/tnino/anaconda3/envs/deeplearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1999: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(categories_from_tensor(target).numpy(), predicted.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32a811",
   "metadata": {},
   "source": [
    "#### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4d5ed533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(dataloader, model, optimizer, loss):\n",
    "    assert model.training, \"model should be in training mode\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Run train loop in ${device}\")\n",
    "    running_loss = []\n",
    "    model.to(device)\n",
    "    for idx, batch in enumerate(tqdm(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_data = batch[\"data\"].to(device)\n",
    "        output = model(input_data)\n",
    "\n",
    "        target_data = batch[\"target\"].to(device)\n",
    "        \n",
    "        target = cmp(target_data).to(device)\n",
    "                            \n",
    "        loss_value = loss(output, target)\n",
    "        loss_value.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss_value.item())\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c21682e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd94e82c02a24144abe1f2e3e0017066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run train loop in $cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8003216ae4b4b2da4c2673a85bb5086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = optim.Adam(m.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "loss = nn.BCELoss()\n",
    "m.train()\n",
    "for epoch in trange(1):\n",
    "    run_train(test_dataloader, m, optimizer, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71b955",
   "metadata": {},
   "source": [
    "#### Loop de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b36d7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(dataloader, model, loss):\n",
    "    assert not model.training, \"model should not be in training mode\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Run eval loop in ${device}\")\n",
    "    running_loss = []\n",
    "    targets = []\n",
    "    predictions = []\n",
    "                     \n",
    "    model.to(device)\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_data = batch[\"data\"].to(device)\n",
    "        output = model(input_data)\n",
    "                    \n",
    "        target = batch[\"target\"]\n",
    "        target_t = cmp(target).to(device)\n",
    "        \n",
    "        running_loss.append(\n",
    "            loss(output, target_t).item()\n",
    "        )\n",
    "        \n",
    "        predicted = categories_from_tensor(output)\n",
    "        y_true = categories_from_tensor(target)\n",
    "\n",
    "        targets.extend(y_true.cpu().numpy())\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "    return running_loss, targets, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.BCELoss()\n",
    "# m.eval()\n",
    "\n",
    "# for epoch in trange(1):\n",
    "#     _, targets, predictions = run_eval(test_dataloader, m, loss)\n",
    "#     print(balanced_accuracy_score(targets, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea32b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b26b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4be2296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(params, **kwargs):\n",
    "    \"\"\"\n",
    "    Run a single experiment\n",
    "\n",
    "    params: A list of (model, loss_fn, optimizer,num_epochs) tuples\n",
    "    \"\"\"\n",
    "    for(build_model, build_loss, build_optimizer, nepochs, batch_sizes) in params:\n",
    "\n",
    "        # Pasamos varios valores para epochs, ejecutamos un experimento para cada uno\n",
    "        for epochs in nepochs:\n",
    "            for bs in batch_sizes:\n",
    "                \n",
    "                print(f\"Experiment epochs:{epochs}, batch-size:{bs}\")\n",
    "                train_dataloader = get_train_dataloader(bs)\n",
    "                test_dataloader = get_test_dataloader(bs)\n",
    "                \n",
    "\n",
    "                with mlflow.start_run():\n",
    "                    # Crea nuevos objetos para cada epoca\n",
    "                    model = build_model()\n",
    "                    optimizer = build_optimizer(model)\n",
    "                    loss = build_loss()\n",
    "\n",
    "                    exp_name = model.name() + \"_e\" + str(epochs) + \"_b\" + str(bs)\n",
    "                    mlflow.set_experiment(exp_name)\n",
    "\n",
    "                    mlflow.log_param(\"model_name\", model.name())\n",
    "                    mlflow.log_param(\"epochs\", str(epochs))\n",
    "                    mlflow.log_param(\"batch_size\", str(bs))\n",
    "\n",
    "                    model.to(device)\n",
    "                    for epoch in trange(epochs):\n",
    "                        model.train()\n",
    "                        running_loss = run_train(train_dataloader, model, optimizer, loss)\n",
    "                        mlflow.log_metric(\"train_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "\n",
    "                        model.eval()\n",
    "                        running_loss, targets, predictions = run_eval(test_dataloader, model, loss)\n",
    "                        mlflow.log_metric(\"test_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "                        mlflow.log_metric(\"test_avp\", balanced_accuracy_score(targets, predictions), epoch)\n",
    "\n",
    "                    print(f\"Save results. epochs:{epochs}, batch-size:{bs}\")\n",
    "                    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "                        targets = []\n",
    "                        predictions = []\n",
    "\n",
    "                        for batch in tqdm(test_dataloader):\n",
    "                            output = model(batch[\"data\"].to(device))\n",
    "                            \n",
    "                            y_true = categories_from_tensor(batch[\"target\"])\n",
    "                            targets.extend(y_true.numpy())\n",
    "                            \n",
    "                            predicted = categories_from_tensor(output)\n",
    "                            predictions.extend(predicted.squeeze().detach().cpu().numpy())\n",
    "\n",
    "                        filename = \"{}/{}_predictions.csv.gz\".format(tmpdirname, exp_name)\n",
    "                        pd.DataFrame({\"prediction\": predictions, \"target\": targets}).to_csv(\n",
    "                            filename, index=False\n",
    "                        )\n",
    "                        mlflow.log_artifact(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9c2cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80051c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lambda : nn.BCELoss()\n",
    "\n",
    "def getOptimizer(model, lr = 1e-3, wd = 1e-5):\n",
    "    return optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "                   \n",
    "epochs = [1, 2, 4]\n",
    "batch_sizes = [24, 48, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a41840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_test_experiment = [(lambda : MLP(20, output_size, hidden = [256]), loss, getOptimizer, [1], [4])]\n",
    "#run_experiment(mlp_test_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mlp_single_layer = [(lambda : MLP(20, output_size, hidden = [256]), loss, getOptimizer, epochs, batch_sizes)]\n",
    "mlp_multiple_layers = [(lambda : MLP(20, output_size, hidden = [40,64,256, 512]), loss, getOptimizer, [20], [15,30])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_experiment(mlp_single_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_experiment(mlp_multiple_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e9d75aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment epochs:20, batch-size:160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/11/13 13:34:49 INFO mlflow.tracking.fluent: Experiment with name 'MLP_50_1024_632_e20_b160' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0d2b1a02394dd2b149e0597275b135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run train loop in $cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d8b4c81847494d981ba6db0eefec1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2559745/2449536598.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2559745/3243884792.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(params, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                         \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2559745/1804678906.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(dataloader, model, optimizer, loss)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtarget_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2559745/2630340146.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "run_experiment([(lambda : MLP(50, output_size, hidden = [1024]), loss, getOptimizer, [20], [160])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c397b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
