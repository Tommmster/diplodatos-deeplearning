{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36c6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn import metrics\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import functools\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tempfile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, balanced_accuracy_score\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing import preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torchmetrics import AveragePrecision\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "cc3b9dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1+cu111'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "6d42351f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿CUDA disponible? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"¿CUDA disponible? {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acece3d9",
   "metadata": {},
   "source": [
    "## Exploración de datos\n",
    "Cargamos los datos de entrenamiento para ver que forma tienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "3a2f272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = pd.read_json('./data/meli-challenge-2019/spanish.train.jsonl.gz',lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a4c19e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>label_quality</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "      <th>n_labels</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spanish</td>\n",
       "      <td>reliable</td>\n",
       "      <td>Casita Muñecas Barbies Pintadas</td>\n",
       "      <td>DOLLHOUSES</td>\n",
       "      <td>train</td>\n",
       "      <td>[casita, muñecas, barbies, pintadas]</td>\n",
       "      <td>[50001, 2, 50000, 3]</td>\n",
       "      <td>0</td>\n",
       "      <td>632</td>\n",
       "      <td>4895280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Neceser Cromado Holográfico</td>\n",
       "      <td>TOILETRY_BAGS</td>\n",
       "      <td>train</td>\n",
       "      <td>[neceser, cromado, holográfico]</td>\n",
       "      <td>[6, 4, 5]</td>\n",
       "      <td>1</td>\n",
       "      <td>632</td>\n",
       "      <td>4895280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Funda Asiento A Medida D20 Chevrolet</td>\n",
       "      <td>CAR_SEAT_COVERS</td>\n",
       "      <td>train</td>\n",
       "      <td>[funda, asiento, medida, chevrolet]</td>\n",
       "      <td>[9, 7, 10, 8]</td>\n",
       "      <td>2</td>\n",
       "      <td>632</td>\n",
       "      <td>4895280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spanish</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>Embrague Ford Focus One 1.8 8v Td (90cv) Desde...</td>\n",
       "      <td>AUTOMOTIVE_CLUTCH_KITS</td>\n",
       "      <td>train</td>\n",
       "      <td>[embrague, ford, focus, one]</td>\n",
       "      <td>[11, 13, 12, 14]</td>\n",
       "      <td>3</td>\n",
       "      <td>632</td>\n",
       "      <td>4895280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language label_quality                                              title  \\\n",
       "0  spanish      reliable                    Casita Muñecas Barbies Pintadas   \n",
       "1  spanish    unreliable                       Neceser Cromado Holográfico    \n",
       "2  spanish    unreliable               Funda Asiento A Medida D20 Chevrolet   \n",
       "3  spanish    unreliable  Embrague Ford Focus One 1.8 8v Td (90cv) Desde...   \n",
       "\n",
       "                 category  split                       tokenized_title  \\\n",
       "0              DOLLHOUSES  train  [casita, muñecas, barbies, pintadas]   \n",
       "1           TOILETRY_BAGS  train       [neceser, cromado, holográfico]   \n",
       "2         CAR_SEAT_COVERS  train   [funda, asiento, medida, chevrolet]   \n",
       "3  AUTOMOTIVE_CLUTCH_KITS  train          [embrague, ford, focus, one]   \n",
       "\n",
       "                   data  target  n_labels     size  \n",
       "0  [50001, 2, 50000, 3]       0       632  4895280  \n",
       "1             [6, 4, 5]       1       632  4895280  \n",
       "2         [9, 7, 10, 8]       2       632  4895280  \n",
       "3      [11, 13, 12, 14]       3       632  4895280  "
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_dataset.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3106307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c98af1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.label_quality.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc03df",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Tomado de https://github.com/DiploDatos/AprendizajeProfundo/blob/master/3_datasets.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "da2cce38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.9999, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros(632)\n",
    "x[1] = 1 - 1e-4\n",
    "torch.Tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428e51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiChallengeDataset(IterableDataset):\n",
    "    def __init__(self, path, transform=None, key = 'title'):\n",
    "        \"\"\"\n",
    "        path: Ubicación a los datos (comprimidos con gzip)\n",
    "        key: Columna que vamos a usar para entrenar\n",
    "        \"\"\"\n",
    "        self.dataset_path = path\n",
    "        self.transform = transform\n",
    "        self.key = key\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Habilita un iterador sobre los datos\n",
    "        \"\"\"\n",
    "        with gzip.open(self.dataset_path, \"rt\") as fh:\n",
    "            for l in fh:\n",
    "                data = json.loads(l)    \n",
    "                \n",
    "                # one-hot encoding like.\n",
    "                index = int(data['target'])\n",
    "                encoded_target = np.zeros(632)\n",
    "                encoded_target[index] = 1\n",
    "\n",
    "                item = {\n",
    "                    \"data\": data[self.key],\n",
    "#                     \"target\": data['target']\n",
    "                    \"target\": encoded_target #data['target']\n",
    "                }\n",
    "                \n",
    "                if self.transform:\n",
    "                    yield self.transform(item)\n",
    "                else:\n",
    "                    yield item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd1d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "        \n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "            \n",
    "        return {\n",
    "#            \"data\": torch.LongTensor(data),\n",
    "            \"data\": torch.FloatTensor(data), \n",
    "            \"target\": torch.FloatTensor(target)\n",
    "            \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ce9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una collate_fn que nos retorne todos los arreglos de la misma longitud\n",
    "# data_len = train_dataset.data.apply(lambda v: len(v))\n",
    "# max_len = data_len.max()\n",
    "\n",
    "\n",
    "pad_to_len = PadSequences(max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e15a3",
   "metadata": {},
   "source": [
    "### Juntando todo\n",
    "Tenemos\n",
    "* Una clase con la responsabilidad de entregar datos, potencialmente preprocesandolos si hace falta\n",
    "* Una función que transforma los datos leidos para que todos los valores tengan la misma longitud\n",
    "\n",
    "A partir de esto, creamos dos instancia del `DataLoader`: Uno para cargar los datos de entrenamiento y otro para cargar los datos de _test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16108441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MeLiChallengeDataset('./data/meli-challenge-2019/spanish.train.jsonl.gz', key = 'data')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn = pad_to_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d91f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MeLiChallengeDataset('./data/meli-challenge-2019/spanish.validation.jsonl.gz', key = 'data') \n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn = pad_to_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc4460",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "Tomamos el MLP de las clases y agregamos los parametros que nos interesan\n",
    "\n",
    "* Capa de entrada: Tantas neuronas como tokens\n",
    "* Capa de salida: Tantas neuronas como categorías (632)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0140e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden):\n",
    "        \"\"\"\n",
    "        input_size: Número de neuronas de entrada\n",
    "        output_size: Número de neuronas de salida\n",
    "        hidden: Lista con los numeros de capas ocultas\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(hidden) > 0\n",
    "        \n",
    "        self._name = str(input_size) +'_'+ \"_\".join(map(lambda i: str(i), hidden)) + '_' + str(output_size)\n",
    "        \n",
    "        neurons = [input_size]  + hidden \n",
    "        parts = []\n",
    "        for idx, each in enumerate(neurons[:-1]):\n",
    "            parts.append(nn.Linear(each, neurons[idx + 1]))\n",
    "            parts.append(nn.ReLU())\n",
    "            \n",
    "        parts = parts + [nn.Linear(neurons[-1], output_size), nn.Sigmoid()]\n",
    "        \n",
    "        self.model = nn.Sequential(*parts)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MLP_\" + self._name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03b08c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLP_10_16_2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP(10, 2 , hidden = [16]).name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ce89f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=632, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP(20,632,[256])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c773c0",
   "metadata": {},
   "source": [
    "### Armado paso a paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe899a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias para obtener los datos en batch\n",
    "get_train_dataloader = lambda bs: DataLoader(train_dataset, batch_size=bs, shuffle=False, num_workers=0, collate_fn = pad_to_len)\n",
    "get_test_dataloader = lambda bs: DataLoader(test_dataset, batch_size=bs, shuffle=False, num_workers=0, collate_fn = pad_to_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3687c",
   "metadata": {},
   "source": [
    "#### Verificamos la forma de la entrada, salida y la etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff8f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1857091/1489816806.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  \"target\": torch.FloatTensor(target)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[5.0001e+04, 2.0000e+00, 5.0000e+04, 3.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [6.0000e+00, 4.0000e+00, 5.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [9.0000e+00, 7.0000e+00, 1.0000e+01, 8.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.1000e+01, 1.3000e+01, 1.2000e+01, 1.4000e+01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]]),\n",
       " tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(train_dataloader)\n",
    "e = it.next()\n",
    "data, target = e['data'], e['target']\n",
    "data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b296f522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 20]), torch.Size([4, 632]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size(), target.size() # batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c2fe574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3482, 0.6162, 0.3838,  ..., 0.3217, 0.4873, 0.4492],\n",
       "         [0.1376, 0.6017, 0.2552,  ..., 0.2127, 0.3714, 0.4205],\n",
       "         [0.0710, 0.5903, 0.1210,  ..., 0.1555, 0.3135, 0.4006]],\n",
       "        grad_fn=<SigmoidBackward0>),\n",
       " tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = MLP(20, 632,[256])\n",
    "output = m(data)\n",
    "output, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "797bc1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertimos a la forma esperada\n",
    "#cmp = lambda t: torch.Tensor([[x] for x in t])\n",
    "cmp = lambda t: t #ID\n",
    "actual = cmp(target)\n",
    "actual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ee7cf",
   "metadata": {},
   "source": [
    "#### Verificamos que se pueda calcular la función de pérdida y el _score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "e8629c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.4385, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.BCELoss()\n",
    "loss_value = loss(output, actual)\n",
    "loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "06c8cca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_from_tensor = lambda t: torch.max(t.data, 1)[1]\n",
    "y_true = categories_from_tensor(target)\n",
    "y_true.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e0cafd09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0, 533, 533, 545]),\n",
       " array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = categories_from_tensor(output)\n",
    "predicted.numpy(), target.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d8a341c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/tnino/anaconda3/envs/deeplearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1999: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(categories_from_tensor(target).numpy(), predicted.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32a811",
   "metadata": {},
   "source": [
    "#### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d5ed533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(dataloader, model, optimizer, loss):\n",
    "    assert model.training, \"model should be in training mode\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Run train loop in ${device}\")\n",
    "    running_loss = []\n",
    "    model.to(device)\n",
    "    for idx, batch in enumerate(tqdm(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_data = batch[\"data\"].to(device)\n",
    "        output = model(input_data)\n",
    "\n",
    "        target_data = batch[\"target\"].to(device)\n",
    "        \n",
    "        target = cmp(target_data).to(device)\n",
    "                            \n",
    "        loss_value = loss(output, target)\n",
    "        loss_value.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss_value.item())\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "9c21682e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f8f331cd9c45e5b01f25504edf22dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run train loop in $cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c6330953ca4563944549cc146ec437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2297025/737694025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2297025/1804678906.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(dataloader, model, optimizer, loss)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mrunning_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.9/site-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_enter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(m.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "loss = nn.BCELoss()\n",
    "m.train()\n",
    "for epoch in trange(1):\n",
    "    run_train(test_dataloader, m, optimizer, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71b955",
   "metadata": {},
   "source": [
    "#### Loop de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b36d7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(dataloader, model, loss):\n",
    "    assert not model.training, \"model should not be in training mode\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Run eval loop in ${device}\")\n",
    "    running_loss = []\n",
    "    targets = []\n",
    "    predictions = []\n",
    "                     \n",
    "    model.to(device)\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_data = batch[\"data\"].to(device)\n",
    "        output = model(input_data)\n",
    "                    \n",
    "        target = batch[\"target\"]\n",
    "        target_t = cmp(target).to(device)\n",
    "        \n",
    "        running_loss.append(\n",
    "            loss(output, target_t).item()\n",
    "        )\n",
    "        \n",
    "        predicted = categories_from_tensor(output)\n",
    "        y_true = categories_from_tensor(target)\n",
    "\n",
    "        targets.extend(y_true.cpu().numpy())\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "    return running_loss, targets, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "m.eval()\n",
    "\n",
    "for epoch in trange(1):\n",
    "    _, targets, predictions = run_eval(test_dataloader, m, loss)\n",
    "    print(balanced_accuracy_score(targets, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea32b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b26b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4be2296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(params, **kwargs):\n",
    "    \"\"\"\n",
    "    Run a single experiment\n",
    "\n",
    "    params: A list of (model, loss_fn, optimizer,num_epochs) tuples\n",
    "    \"\"\"\n",
    "    for(build_model, build_loss, build_optimizer, nepochs, batch_sizes) in params:\n",
    "\n",
    "        # Pasamos varios valores para epochs, ejecutamos un experimento para cada uno\n",
    "        for epochs in nepochs:\n",
    "            for bs in batch_sizes:\n",
    "                \n",
    "                print(f\"Experiment epochs:{epochs}, batch-size:{bs}\")\n",
    "                train_dataloader = get_train_dataloader(bs)\n",
    "                test_dataloader = get_test_dataloader(bs)\n",
    "                \n",
    "\n",
    "                with mlflow.start_run():\n",
    "                    # Crea nuevos objetos para cada epoca\n",
    "                    model = build_model()\n",
    "                    optimizer = build_optimizer(model)\n",
    "                    loss = build_loss()\n",
    "\n",
    "                    exp_name = model.name() + \"_e\" + str(epochs) + \"_b\" + str(bs)\n",
    "                    mlflow.set_experiment(exp_name)\n",
    "\n",
    "                    mlflow.log_param(\"model_name\", model.name())\n",
    "                    mlflow.log_param(\"epochs\", str(epochs))\n",
    "                    mlflow.log_param(\"batch_size\", str(bs))\n",
    "\n",
    "                    model.to(device)\n",
    "                    for epoch in trange(epochs):\n",
    "                        model.train()\n",
    "                        running_loss = run_train(train_dataloader, model, optimizer, loss)\n",
    "                        mlflow.log_metric(\"train_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "\n",
    "                        model.eval()\n",
    "                        running_loss, targets, predictions = run_eval(test_dataloader, model, loss)\n",
    "                        mlflow.log_metric(\"test_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "                        mlflow.log_metric(\"test_avp\", balanced_accuracy_score(targets, predictions), epoch)\n",
    "\n",
    "                    print(f\"Save results. epochs:{epochs}, batch-size:{bs}\")\n",
    "                    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "                        targets = []\n",
    "                        predictions = []\n",
    "\n",
    "                        for batch in tqdm(test_dataloader):\n",
    "                            output = model(batch[\"data\"].to(device))\n",
    "                            \n",
    "                            y_true = categories_from_tensor(batch[\"target\"])\n",
    "                            targets.extend(y_true.numpy())\n",
    "                            \n",
    "                            predicted = categories_from_tensor(output)\n",
    "                            predictions.extend(predicted.squeeze().detach().cpu().numpy())\n",
    "\n",
    "                        filename = \"{}/{}_predictions.csv.gz\".format(tmpdirname, exp_name)\n",
    "                        pd.DataFrame({\"prediction\": predictions, \"target\": targets}).to_csv(\n",
    "                            filename, index=False\n",
    "                        )\n",
    "                        mlflow.log_artifact(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9c2cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80051c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lambda : nn.BCELoss()\n",
    "\n",
    "def getOptimizer(model, lr = 1e-3, wd = 1e-5):\n",
    "    return optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "                   \n",
    "epochs = [1, 2, 4]\n",
    "batch_sizes = [24, 48, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a41840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_test_experiment = [(lambda : MLP(20, output_size, hidden = [256]), loss, getOptimizer, [1], [4])]\n",
    "run_experiment(mlp_test_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca1c1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mlp_single_layer = [(lambda : MLP(20, output_size, hidden = [256]), loss, getOptimizer, epochs, batch_sizes)]\n",
    "mlp_multiple_layers = [(lambda : MLP(20, output_size, hidden = [40,64,256, 512]), loss, getOptimizer, epochs, batch_sizes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(mlp_single_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f69b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment epochs:1, batch-size:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/23 10:28:16 INFO mlflow.tracking.fluent: Experiment with name 'MLP_20_40_64_256_512_632_e1_b24' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab95efbba55478fbde2bdef08011d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1857091/184097019.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_multiple_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1857091/3243884792.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(params, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                         \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_train' is not defined"
     ]
    }
   ],
   "source": [
    "run_experiment(mlp_multiple_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
