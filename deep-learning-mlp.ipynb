{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c36c6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn import metrics\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import functools\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tempfile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, balanced_accuracy_score\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing import preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torchmetrics import AveragePrecision\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc3b9dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1+cu111'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d42351f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿CUDA disponible? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"¿CUDA disponible? {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acece3d9",
   "metadata": {},
   "source": [
    "## Exploración de datos\n",
    "Cargamos los datos de entrenamiento para ver que forma tienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a2f272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = pd.read_json('./data/meli-challenge-2019/spanish.train.jsonl.gz',lines=True)\n",
    "# train_dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3106307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c98af1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.label_quality.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc03df",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Tomado de https://github.com/DiploDatos/AprendizajeProfundo/blob/master/3_datasets.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "428e51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeLiChallengeDataset(IterableDataset):\n",
    "    def __init__(self, path, transform=None, key = 'title'):\n",
    "        \"\"\"\n",
    "        path: Ubicación a los datos (comprimidos con gzip)\n",
    "        key: Columna que vamos a usar para entrenar\n",
    "        \"\"\"\n",
    "        self.dataset_path = path\n",
    "        self.transform = transform\n",
    "        self.key = key\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Habilita un iterador sobre los datos\n",
    "        \"\"\"\n",
    "        with gzip.open(self.dataset_path, \"rt\") as fh:\n",
    "            for l in fh:\n",
    "                data = json.loads(l)\n",
    "                item = {\n",
    "                    \"data\": data[self.key],\n",
    "                    \"target\": data['target']\n",
    "                }\n",
    "                \n",
    "                if self.transform:\n",
    "                    yield self.transform(item)\n",
    "                else:\n",
    "                    yield item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd1d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "        \n",
    "\n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "            \n",
    "        return {\n",
    "#            \"data\": torch.LongTensor(data),\n",
    "            \"data\": torch.FloatTensor(data), \n",
    "            \"target\": torch.FloatTensor(target)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ce9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una collate_fn que nos retorne todos los arreglos de la misma longitud\n",
    "# data_len = train_dataset.data.apply(lambda v: len(v))\n",
    "# max_len = data_len.max()\n",
    "\n",
    "pad_to_len = PadSequences(max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e15a3",
   "metadata": {},
   "source": [
    "### Juntando todo\n",
    "Tenemos\n",
    "* Una clase con la responsabilidad de entregar datos, potencialmente preprocesandolos si hace falta\n",
    "* Una función que transforma los datos leidos para que todos los valores tengan la misma longitud\n",
    "\n",
    "A partir de esto, creamos dos instancia del `DataLoader`: Uno para cargar los datos de entrenamiento y otro para cargar los datos de _test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16108441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MeLiChallengeDataset('./data/meli-challenge-2019/spanish.train.jsonl.gz', key = 'data')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn = pad_to_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52d91f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MeLiChallengeDataset('./data/meli-challenge-2019/spanish.validation.jsonl.gz', key = 'data') \n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn = pad_to_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc4460",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "Tomamos el MLP de las clases y agregamos los parametros que nos interesan\n",
    "\n",
    "* Capa de entrada: Tantas neuronas como tokens\n",
    "* Capa de salida: Tantas neuronas como categorías (632)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0140e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden = []):\n",
    "        \"\"\"\n",
    "        input_size: Número de neuronas de entrada\n",
    "        output_size: Número de neuronas de salida\n",
    "        hidden: Lista con los numeros de capas ocultas\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(hidden) > 0\n",
    "        \n",
    "        self._name = str(input_size) +'_'+ \"_\".join(map(lambda i: str(i), hidden)) + '_' + str(output_size)\n",
    "        \n",
    "        neurons = [input_size]  + hidden \n",
    "        parts = []\n",
    "        for idx, each in enumerate(neurons[:-1]):\n",
    "            parts.append(nn.Linear(each, neurons[idx + 1]))\n",
    "            parts.append(nn.ReLU())\n",
    "            \n",
    "        parts = parts + [nn.Linear(neurons[-1], 1), nn.Sigmoid()]\n",
    "        \n",
    "        self.model = nn.Sequential(*parts)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MLP_\" + self._name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45025603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLP_10_16_2'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP(10, 2 , hidden = [16]).name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c21682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d466303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69aa5a13d2148deabd18abab3924264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8403f4dc9af84a8ca4a021adcc282376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f560ef24270846adae7595b90c69d220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3617633/276871689.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"MLP Basico\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model_name\", \"mlp\") # TODO Log parameters\n",
    "    mlflow.log_param(\"epochs\", \"1\")\n",
    "    mlflow.log_param(\"hidden_layer_1_neurons\", \"2\")\n",
    "    \n",
    "    model = MLP(20, 632, hidden= [2]) # 20 -> 2 -> 632\n",
    "    loss = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    \n",
    "#     model.to(device)\n",
    "    for epoch in trange(1): # TODO Pruebas con varias epochs\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "#             input_data = batch[\"data\"].to(device)\n",
    "            input_data = batch[\"data\"]\n",
    "#             output = model(batch[\"data\"])\n",
    "            output = model(input_data)\n",
    "    \n",
    "#             target_data = batch[\"target\"].view(-1,1).to(device)\n",
    "            target_data = batch[\"target\"].view(-1,1)\n",
    "            loss_value = loss(output, target_data)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss_value.item())        \n",
    "        mlflow.log_metric(\"train_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = []\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            output = model(batch[\"data\"])\n",
    "            running_loss.append(\n",
    "                loss(output, batch[\"target\"].view(-1, 1)).item()\n",
    "            )\n",
    "            targets.extend(batch[\"target\"].numpy())\n",
    "            predictions.extend(output.squeeze().detach().numpy())\n",
    "        mlflow.log_metric(\"test_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "        # Reemplazamos la siguiente metrica por la de pythorch (La de scikit learn no acepta problemas multiclase)\n",
    "\n",
    "        mlflow.log_metric(\"test_avp\", balanced_accuracy_score(targets, predictions), epoch)        \n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "            targets = []\n",
    "            predictions = []\n",
    "            for batch in tqdm(test_dataloader):\n",
    "                output = model(batch[\"data\"])\n",
    "                targets.extend(batch[\"target\"].numpy())\n",
    "                predictions.extend(output.squeeze().detach().numpy())\n",
    "            pd.DataFrame({\"prediction\": predictions, \"target\": targets}).to_csv(\n",
    "                f\"{tmpdirname}/predictions.csv.gz\", index=False\n",
    "            )\n",
    "            mlflow.log_artifact(f\"{tmpdirname}/predictions.csv.gz\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bbe1f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_train_dataloader = lambda bs: DataLoader(train_dataset, batch_size=bs, shuffle=False, num_workers=0, collate_fn = pad_to_len)\n",
    "get_test_dataloader = lambda bs: DataLoader(test_dataset, batch_size=bs, shuffle=False, num_workers=0, collate_fn = pad_to_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3cd22650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(params, **kwargs):\n",
    "    \"\"\"\n",
    "    Run a single experiment\n",
    "\n",
    "    params: A list of (model, loss_fn, optimizer,num_epochs) tuples\n",
    "    \"\"\"\n",
    "    for(build_model, build_loss, build_optimizer, nepochs, batch_sizes) in params:\n",
    "\n",
    "        # Pasamos varios valores para epochs, ejecutamos un experimento para cada uno\n",
    "        for epochs in nepochs:\n",
    "            for bs in batch_sizes:\n",
    "                \n",
    "                print(f\"Experiment epochs:{epochs}, batch-size:{bs}\")\n",
    "                train_dataloader = get_train_dataloader(bs)\n",
    "                test_dataloader = get_test_dataloader(bs)\n",
    "                \n",
    "                with mlflow.start_run():\n",
    "                    # Crea nuevos objetos para cada epoca\n",
    "                    model = build_model()\n",
    "                    optimizer = build_optimizer(model)\n",
    "                    loss = build_loss()\n",
    "\n",
    "                    exp_name = model.name() + \"_e\" + str(epochs) + \"_b\" + str(bs)\n",
    "                    mlflow.set_experiment(exp_name)\n",
    "\n",
    "                    mlflow.log_param(\"model_name\", model.name())\n",
    "                    mlflow.log_param(\"epochs\", str(epochs))\n",
    "                    mlflow.log_param(\"batch_size\", str(bs))\n",
    "\n",
    "                    model.to(device)\n",
    "                    for epoch in trange(epochs):\n",
    "\n",
    "                        model.train()\n",
    "                        running_loss = []\n",
    "                        for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                            input_data = batch[\"data\"].to(device)\n",
    "                            output = model(input_data)\n",
    "\n",
    "                            target_data = batch[\"target\"].view(-1,1).to(device)\n",
    "                            loss_value = loss(output, target_data)\n",
    "                            loss_value.backward()\n",
    "\n",
    "                            optimizer.step()\n",
    "                            running_loss.append(loss_value.item())\n",
    "                        mlflow.log_metric(\"train_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "\n",
    "                        model.eval()\n",
    "                        running_loss = []\n",
    "                        targets = []\n",
    "                        predictions = []\n",
    "                        for batch in tqdm(test_dataloader):\n",
    "                            output = model(batch[\"data\"])\n",
    "                            running_loss.append(\n",
    "                                loss(output, batch[\"target\"].view(-1, 1)).item()\n",
    "                            )\n",
    "                            targets.extend(batch[\"target\"].numpy())\n",
    "                            predictions.extend(output.squeeze().detach().numpy())\n",
    "\n",
    "                        mlflow.log_metric(\"test_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "                        mlflow.log_metric(\"test_avp\", balanced_accuracy_score(targets, predictions), epoch)\n",
    "\n",
    "                  #\n",
    "                  #\n",
    "                    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "                        targets = []\n",
    "                        predictions = []\n",
    "\n",
    "                        for batch in tqdm(test_dataloader):\n",
    "                            output = model(batch[\"data\"])\n",
    "                            targets.extend(batch[\"target\"].numpy())\n",
    "                            predictions.extend(output.squeeze().detach().numpy())\n",
    "\n",
    "                        filename = \"{}/{}_predictions.csv.gz\".format(tmpdirname, exp_name)\n",
    "                        pd.DataFrame({\"prediction\": predictions, \"target\": targets}).to_csv(\n",
    "                            filename, index=False\n",
    "                        )\n",
    "                        mlflow.log_artifact(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc54af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/18 00:13:39 INFO mlflow.tracking.fluent: Experiment with name 'MLP_20_256_632_e1_b4' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment epochs:1, batch-size:4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f5e0b91a04430a8e6a6c66455e9718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3075dd7a474402a774b9fac0a16d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = lambda : nn.BCELoss()\n",
    "\n",
    "def getOptimizer(model, lr = 1e-3, wd = 1e-5):\n",
    "    return optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "mlp = lambda : MLP(20, 632, hidden = [2])\n",
    "                   \n",
    "epochs = [1, 20, 40]\n",
    "batch_sizes = [4,16, 48, 64]\n",
    "\n",
    "\n",
    "mlp_single_layer = [(lambda : MLP(20, 632, hidden = [256]), loss, getOptimizer, epochs, batch_sizes)]\n",
    "mlp_multiple_layers = [(lambda : MLP(20, 632, hidden = [40,64,256, 512]), loss, getOptimizer, epochs, batch_sizes)]\n",
    "\n",
    "run_experiment(mlp_single_layer + mlp_multiple_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a8ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
